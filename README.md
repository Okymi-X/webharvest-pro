# WebHarvest Pro ğŸŒ

Solution professionnelle et Ã©thique de web scraping avec interface graphique, dÃ©veloppÃ©e en Python. Cet outil offre une extraction intelligente des donnÃ©es, une gestion optimisÃ©e des connexions et une interface utilisateur conviviale.

## âœ¨ FonctionnalitÃ©s

- ğŸ–¥ï¸ **Interface Graphique AvancÃ©e**
  - Suivi en temps rÃ©el de la progression
  - ParamÃ¨tres de scraping configurables
  - Gestion de l'historique des URLs
  - Tableau de bord statistique en direct
  - Filtrage des niveaux de logs

- ğŸ”„ **Gestion Intelligente des Connexions**
  - Pool de connexions avec mise Ã  l'Ã©chelle automatique
  - MÃ©canisme de rÃ©essai intelligent
  - DÃ©lais configurables entre les requÃªtes
  - Gestion automatique des sessions

- ğŸ§  **DÃ©tection Intelligente des DonnÃ©es**
  - DÃ©tection automatique de la structure
  - Extraction des emails et numÃ©ros de tÃ©lÃ©phone
  - DÃ©tection des liens de rÃ©seaux sociaux
  - Identification des donnÃ©es sensibles

- ğŸ›¡ï¸ **Protection IntÃ©grÃ©e**
  - Respect du fichier robots.txt
  - Mesures anti-dÃ©tection de bot
  - Limitation du taux de requÃªtes
  - Rotation des User-Agents

- ğŸ’¾ **Gestion des DonnÃ©es**
  - Export au format JSON
  - Sortie de donnÃ©es structurÃ©e
  - Sauvegarde de la progression
  - Persistance de la configuration

## ğŸš€ Pour Commencer

### PrÃ©requis

- Python 3.8+
- Navigateur Chrome installÃ©
- Windows/Linux/MacOS

### Installation

1. Clonez le dÃ©pÃ´t :
```bash
git clone https://github.com/votre-nom/webharvest-pro.git
cd webharvest-pro
```

2. CrÃ©ez un environnement virtuel :
```bash
python -m venv venv
source venv/bin/activate  # Sous Windows : venv\Scripts\activate
```

3. Installez les dÃ©pendances :
```bash
pip install -r requirements.txt
```

### Utilisation

1. Lancez l'application graphique :
```bash
python gui.py
```

2. Entrez l'URL cible et configurez les paramÃ¨tres :
   - Pages Max : Limite du nombre de pages Ã  scraper
   - Profondeur Max : DÃ©finit la profondeur de suivi des liens
   - Workers : Nombre de connexions parallÃ¨les (recommandÃ© : 3-5)

3. Options avancÃ©es :
   - Liens Externes : Activer/dÃ©sactiver le suivi des liens externes
   - Robots.txt : Activer/dÃ©sactiver le respect du robots.txt
   - DÃ©lai RequÃªtes : DÃ©finir le dÃ©lai entre les requÃªtes

4. Cliquez sur "DÃ©marrer" pour commencer le scraping

## ğŸ› ï¸ Configuration

### ParamÃ¨tres de Base
- **URL d'EntrÃ©e** : Saisissez l'URL du site cible
- **Pages Max** : Limitez le nombre total de pages
- **Profondeur Max** : ContrÃ´lez la profondeur d'exploration
- **Workers** : DÃ©finissez les connexions parallÃ¨les

### ParamÃ¨tres AvancÃ©s
- **Liens Externes** : Basculez le suivi des liens externes
- **Robots.txt** : Activez/dÃ©sactivez la conformitÃ©
- **DÃ©lai RequÃªtes** : Configurez le dÃ©lai inter-requÃªtes
- **Pool de Connexions** : Configurez la taille du pool

## ğŸ“Š Format des DonnÃ©es

Le scraper sauvegarde les donnÃ©es au format JSON avec la structure suivante :
```json
{
    "metadata": {
        "base_url": "https://exemple.com",
        "total_pages": 100,
        "timestamp": "2024-02-07T12:00:00"
    },
    "pages": {
        "page_id": {
            "url": "https://exemple.com/page",
            "titre": "Titre de la Page",
            "donnees": {...}
        }
    }
}
```

## ğŸ”§ Optimisation des Performances

Pour des performances optimales :
- RÃ©glez les workers entre 3 et 5 pour la plupart des sites
- Activez les dÃ©lais entre requÃªtes (2-3 secondes recommandÃ©es)
- Utilisez le pool de connexions
- Activez le respect du robots.txt
- Surveillez les ressources systÃ¨me

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  soumettre une Pull Request. Pour les changements majeurs, ouvrez d'abord une issue pour discuter des modifications souhaitÃ©es.

1. Forkez le Projet
2. CrÃ©ez votre Branche de FonctionnalitÃ© (`git checkout -b feature/NouvelleFonctionnalite`)
3. Committez vos Changements (`git commit -m 'Ajout de NouvelleFonctionnalite'`)
4. Poussez vers la Branche (`git push origin feature/NouvelleFonctionnalite`)
5. Ouvrez une Pull Request

## ğŸ“ Licence

Ce projet est sous licence MIT - voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

## âš ï¸ Avertissement

Cet outil est destinÃ© uniquement Ã  des fins Ã©ducatives. Veillez Ã  toujours :
- Respecter le fichier robots.txt des sites
- Suivre les conditions d'utilisation des sites
- ImplÃ©menter des dÃ©lais appropriÃ©s
- Utiliser de maniÃ¨re responsable et Ã©thique

## ğŸ™ Remerciements

- Selenium WebDriver
- BeautifulSoup4
- Python Tkinter
- Et tous les autres contributeurs open source

## ğŸ“§ Contact

[@Okymi-X](https://github.com/Okymi-X)

[https://github.com/Okymi-X/webharvest-pro](https://github.com/Okymi-X/webharvest-pro)